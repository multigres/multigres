// Copyright 2025 Supabase, Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.

syntax = "proto3";

package multipoolermanagerdata;

import "clustermetadata.proto";
import "google/protobuf/duration.proto";
import "google/protobuf/timestamp.proto";

option go_package = "github.com/multigres/multigres/go/pb/multipoolermanagerdata";

// Primary connection information parsed from PostgreSQL's primary_conninfo setting
message PrimaryConnInfo {
  // Primary server hostname
  string host = 1;

  // Primary server port
  int32 port = 2;

  // Database user for replication connection
  string user = 3;

  // Application name for this standby
  string application_name = 4;

  // Raw connection string (includes all parameters)
  string raw = 5;
}

// StandbyReplicationStatus PostgreSQL replication status information
message StandbyReplicationStatus {
  // Last WAL position replayed during recovery (PostgreSQL format: X/XXXXXXXX)
  // Obtained from pg_last_wal_replay_lsn()
  string last_replay_lsn = 1;

  // Last WAL position received and synced to disk by standby (PostgreSQL format: X/XXXXXXXX)
  // Obtained from pg_last_wal_receive_lsn()
  string last_receive_lsn = 2;

  // Result of pg_is_wal_replay_paused()
  bool is_wal_replay_paused = 3;

  // Result of pg_get_wal_replay_pause_state()
  string wal_replay_pause_state = 4;

  // Replication lag (optional, may not always be available)
  google.protobuf.Duration lag = 5;

  // Result of pg_last_xact_replay_timestamp()
  string last_xact_replay_timestamp = 6;

  // Primary connection info parsed from primary_conninfo setting
  PrimaryConnInfo primary_conn_info = 7;

  // WAL receiver status from pg_stat_wal_receiver.status
  // Values: "streaming", "stopping", "starting", "waiting", or empty if no WAL receiver
  // Empty string indicates WAL receiver is not running (0 rows from pg_stat_wal_receiver)
  string wal_receiver_status = 8;
}

// Wait for PostgreSQL server to reach a specific LSN position
message WaitForLSNRequest {
  // Target LSN position to wait for (PostgreSQL LSN format: X/XXXXXXXX)
  string target_lsn = 1;

  // Timeout (zero duration means no timeout)
  google.protobuf.Duration timeout = 2;
}

message WaitForLSNResponse {
}

// Start PostgreSQL replication (calls pg_wal_replay_resume)
message StartReplicationRequest {
}

message StartReplicationResponse {
}

// SetPrimaryConnInfo sets the primary connection info for a standby server
message SetPrimaryConnInfoRequest {
  // Primary pooler metadata. Contains id (cell+name), hostname, and port_map.
  // The postgres port is extracted from port_map["postgres"].
  clustermetadata.MultiPooler primary = 1;

  // Whether to stop replication before making changes
  bool stop_replication_before = 2;

  // Whether to start replication after making changes
  bool start_replication_after = 3;

  // Current term for consensus (used by MultiOrch during initialization)
  int64 current_term = 4;

  // Force the operation even if the term doesn't match
  bool force = 5;
}

message SetPrimaryConnInfoResponse {
}

// StopReplication stops WAL replay on standby
message StopReplicationRequest {
  // Mode specifying what aspect of replication to pause
  ReplicationPauseMode mode = 1;

  // Whether to wait for the pause operation to complete before returning
  // If true, the response will include the replication status after pausing
  // If false, the operation is asynchronous and status will be null
  bool wait = 2;
}

message StopReplicationResponse {
  // Replication status when replication was stopped (only populated if wait=true)
  StandbyReplicationStatus status = 1;
}

// StandbyReplicationStatus gets the current replication status of the standby
message StandbyReplicationStatusRequest {
}

message StandbyReplicationStatusResponse {
  // Current replication status
  StandbyReplicationStatus status = 1;
}

// Synchronous replication configuration
message SynchronousReplicationConfiguration {
  // Synchronous commit level
  SynchronousCommitLevel synchronous_commit = 1;

  // Synchronous method (FIRST or ANY)
  SynchronousMethod synchronous_method = 2;

  // Number of synchronous standbys
  int32 num_sync = 3;

  // List of standby IDs that can participate in synchronous replication
  repeated clustermetadata.ID standby_ids = 4;
}

// PostgreSQL primary server status
message PrimaryStatus {
  // Current WAL LSN position (PostgreSQL format: X/XXXXXXXX)
  string lsn = 1;

  // Whether server is accepting connections
  bool ready = 2;

  // Follower servers that are currently connected to the primary (parsed from pg_stat_replication).
  // This only includes followers that have an active replication connection, not all configured standbys.
  // For a complete view of all configured standbys (connected and disconnected), use GetFollowers.
  repeated clustermetadata.ID connected_followers = 3;

  // Synchronous replication configuration (parsed from synchronous_standby_names and synchronous_commit)
  SynchronousReplicationConfiguration sync_replication_config = 4;

  // The term for which this pooler was promoted to primary.
  // Non-zero for current primaries. See ConsensusTerm.primary_term for full lifecycle details.
  int64 primary_term = 5;
}

// PrimaryStatus gets the status of the primary server
message PrimaryStatusRequest {
}

message PrimaryStatusResponse {
  // Primary server status
  PrimaryStatus status = 1;
}

// PrimaryPosition gets the current LSN position of the primary
message PrimaryPositionRequest {
}

message PrimaryPositionResponse {
  // Current primary LSN position
  string lsn_position = 1;
}

// Status provides unified status information that works for both PRIMARY and REPLICA poolers.
// This RPC works even when the database connection is unavailable - fields that require
// database access will be nil/empty in that case.
message Status {
  // Pooler type from topology (PRIMARY or REPLICA)
  clustermetadata.PoolerType pooler_type = 1;

  // Primary status information (only populated if pooler is acting as primary AND db is connected)
  PrimaryStatus primary_status = 2;

  // Replication status information (only populated if pooler is acting as standby AND db is connected)
  StandbyReplicationStatus replication_status = 3;

  // === Initialization status fields (always available, even without db connection) ===

  // Whether this pooler has been initialized (has data directory and multigres schema)
  bool is_initialized = 4;

  // Whether data directory exists
  bool has_data_directory = 5;

  // Whether PostgreSQL is currently running
  bool postgres_running = 6;

  // Current postgres-level role from pg_is_in_recovery ("primary", "standby", or "unknown")
  // This may differ from pooler_type during transitions or failures.
  string postgres_role = 7;

  // Current WAL position (empty if db not connected)
  string wal_position = 8;

  // Current consensus term information
  ConsensusTerm consensus_term = 9;

  // Shard ID that this pooler belongs to
  string shard_id = 10;
}

// Status gets unified status that works for both PRIMARY and REPLICA poolers
message StatusRequest {
}

message StatusResponse {
  Status status = 1;
}

// Replication statistics for a connected follower from pg_stat_replication
message ReplicationStats {
  // PostgreSQL backend process ID for this replication connection
  int32 pid = 1;

  // Client IP address of the follower
  string client_addr = 2;

  // Current WAL sender state (startup, catchup, streaming, backup, stopping)
  string state = 3;

  // Synchronous state (async, potential, sync, quorum)
  string sync_state = 4;

  // LSN of the last WAL location sent to this standby
  string sent_lsn = 5;

  // LSN of the last WAL location written to disk on this standby
  string write_lsn = 6;

  // LSN of the last WAL location flushed to disk on this standby
  string flush_lsn = 7;

  // LSN of the last WAL location replayed on this standby
  string replay_lsn = 8;

  // Time taken to write recent WAL locally (may be null)
  google.protobuf.Duration write_lag = 9;

  // Time taken to flush recent WAL locally (may be null)
  google.protobuf.Duration flush_lag = 10;

  // Time taken to replay recent WAL on standby (may be null)
  google.protobuf.Duration replay_lag = 11;
}

// Information about a follower that is part of the current cohort (configured in
// synchronous_standby_names). Includes connection status and replication stats if connected.
message FollowerInfo {
  // Cluster ID of the follower from synchronous_standby_names configuration
  clustermetadata.ID follower_id = 1;

  // Application name (format: {cell}_{name})
  string application_name = 2;

  // Whether this follower is currently connected to the primary
  bool is_connected = 3;

  // Replication statistics (only populated if is_connected is true)
  ReplicationStats replication_stats = 4;
}

// GetFollowers gets the list of follower servers with detailed replication status
message GetFollowersRequest {
}

message GetFollowersResponse {
  // Information about all followers configured in synchronous_standby_names.
  // Each entry indicates whether the follower is connected and includes replication
  // statistics if available. This provides a complete view of the expected cohort.
  repeated FollowerInfo followers = 1;

  // Current synchronous replication configuration for context
  SynchronousReplicationConfiguration sync_config = 2;
}

// EmergencyDemote demotes the current leader server
// This is called during the Revocation stage of generalized consensus to safely
// transition a primary to read-only mode and prevent it from making further progress.
message EmergencyDemoteRequest {
  // Consensus term for this demotion operation
  // When force=true, this field is ignored and should be passed as 0
  int64 consensus_term = 1;

  // Drain timeout - how long to wait for in-flight queries (default: 5s)
  google.protobuf.Duration drain_timeout = 2;

  // Force the operation even if term validation fails
  bool force = 3;
}

message EmergencyDemoteResponse {
  // Whether the pooler was already demoted (idempotent check)
  bool was_already_demoted = 1;

  // Consensus term at the time of demotion
  int64 consensus_term = 2;

  // LSN position at the time of demotion (final position as primary)
  string lsn_position = 3;

  // Number of connections that were terminated
  int32 connections_terminated = 4;
}

// DemoteStalePrimary demotes a stale primary that came back online after failover.
// This is a complete operation that:
// 1. Stops postgres if running
// 2. Runs pg_rewind to sync with the correct primary
// 3. Clears sync replication config
// 4. Restarts as standby
// 5. Updates topology to REPLICA
message DemoteStalePrimaryRequest {
  // Source multipooler (the correct primary) to rewind to
  clustermetadata.MultiPooler source = 1;

  // Consensus term from the correct primary
  int64 consensus_term = 2;

  // Force the operation even if term validation fails
  bool force = 3;
}

message DemoteStalePrimaryResponse {
  // True if the operation completed successfully
  bool success = 1;

  // Whether pg_rewind was performed (servers were diverged)
  bool rewind_performed = 2;

  // LSN position after becoming standby
  string lsn_position = 3;
}

// UndoDemote undoes a demotion
message UndoDemoteRequest {
}

message UndoDemoteResponse {
}

// StopReplicationAndGetStatus stops PostgreSQL replication (replay and/or receiver based on mode)
// and returns the status
message StopReplicationAndGetStatusRequest {
  // Mode specifying what aspect of replication to pause
  ReplicationPauseMode mode = 1;

  // Whether to wait for the pause operation to complete before returning
  // If true, the response will include the replication status after pausing
  // If false, the operation is asynchronous and status reflects state when call was made
  bool wait = 2;
}

message StopReplicationAndGetStatusResponse {
  // Replication status after stopping
  StandbyReplicationStatus status = 1;
}

// ChangeType changes the pooler type
message ChangeTypeRequest {
  // New pooler type (PRIMARY or REPLICA - maps to LEADER/FOLLOWER)
  clustermetadata.PoolerType pooler_type = 1;
}

message ChangeTypeResponse {
}

// Promote promotes a replica to leader (Multigres-level operation)
// This is called during the Propagate stage of generalized consensus
// to safely transition a standby to primary and reconfigure replication.
message PromoteRequest {
  // Consensus term for this promotion operation
  // Used to ensure this promotion corresponds to the correct term
  // When force=true, this field is ignored and should be passed as 0
  int64 consensus_term = 1;

  // Expected LSN position before promotion (optional, for validation)
  // By the Propagate stage, replication should already be stopped and the LSN frozen.
  // This is an assertion to verify the pooler has the expected durable state.
  // If the actual LSN doesn't match, this indicates an error in an earlier consensus stage.
  // If empty, skip LSN validation.
  string expected_lsn = 2;

  // Synchronous replication configuration to apply after promotion
  // This rewires the cohort for the new topology
  ConfigureSynchronousReplicationRequest sync_replication_config = 3;

  // Force the operation even if term validation fails
  // Should only be used in recovery scenarios
  bool force = 4;

  // Reason for the election (e.g., "dead_primary", "manual_failover", "bootstrap")
  string reason = 5;

  // Coordinator ID that ran this election
  string coordinator_id = 6;

  // List of pooler names that were in the cohort
  repeated string cohort_members = 7;

  // List of pooler names that accepted the term during BeginTerm
  repeated string accepted_members = 8;
}

message PromoteResponse {
  // LSN position after promotion
  string lsn_position = 1;

  // Whether the pooler was already promoted (idempotent check)
  bool was_already_primary = 2;

  // Consensus term at the time of promotion
  int64 consensus_term = 3;
}

// ResetReplication resets the standby's connection to its primary
message ResetReplicationRequest {
}

message ResetReplicationResponse {
  // Replication status when replication was reset
  StandbyReplicationStatus status = 1;
}

// Replication pause mode - defines what aspect of replication to pause
enum ReplicationPauseMode {
  // PAUSE_REPLAY_ONLY pauses WAL replay only (using pg_wal_replay_pause)
  // The WAL receiver continues to receive data from the primary
  REPLICATION_PAUSE_MODE_REPLAY_ONLY = 0;

  // PAUSE_RECEIVER_ONLY stops the WAL receiver by clearing primary_conninfo
  // Already replayed WAL remains, but no new WAL is received
  REPLICATION_PAUSE_MODE_RECEIVER_ONLY = 1;

  // PAUSE_REPLAY_AND_RECEIVER pauses both WAL replay and stops the WAL receiver
  // This is the most complete pause - no new WAL is received and nothing is replayed
  REPLICATION_PAUSE_MODE_REPLAY_AND_RECEIVER = 2;
}

// Synchronization method for standby servers
enum SynchronousMethod {
  // UNSPECIFIED means no synchronous replication (empty synchronous_standby_names)
  SYNCHRONOUS_METHOD_UNSPECIFIED = 0;

  // FIRST means wait for the first N standby servers (PostgreSQL default)
  SYNCHRONOUS_METHOD_FIRST = 1;

  // ANY means wait for any N standby servers
  SYNCHRONOUS_METHOD_ANY = 2;
}

// Enum representing the type of standby list modification
enum StandbyUpdateOperation {
  STANDBY_UPDATE_OPERATION_UNSPECIFIED = 0;
  STANDBY_UPDATE_OPERATION_ADD = 1;
  STANDBY_UPDATE_OPERATION_REMOVE = 2;
  STANDBY_UPDATE_OPERATION_REPLACE = 3;
}

// Synchronous commit level
enum SynchronousCommitLevel {
  // OFF disables synchronous commit
  SYNCHRONOUS_COMMIT_OFF = 0;

  // LOCAL waits for local flush to disk
  SYNCHRONOUS_COMMIT_LOCAL = 1;

  // REMOTE_WRITE waits for standby to receive and write WAL
  SYNCHRONOUS_COMMIT_REMOTE_WRITE = 2;

  // ON waits for standby to receive, write, and flush WAL (same as remote_apply in newer versions)
  SYNCHRONOUS_COMMIT_ON = 3;

  // REMOTE_APPLY waits for standby to receive, write, flush, and apply WAL
  SYNCHRONOUS_COMMIT_REMOTE_APPLY = 4;
}

// ConfigureSynchronousReplication configures PostgreSQL synchronous replication settings
message ConfigureSynchronousReplicationRequest {
  // Synchronous commit level (synchronous_commit setting)
  SynchronousCommitLevel synchronous_commit = 1;

  // Synchronization method (FIRST, ANY)
  SynchronousMethod synchronous_method = 2;

  // Number of standby servers to wait for (quorum count)
  int32 num_sync = 3;

  // List of standby multipooler IDs that can participate in synchronous replication
  // The application names will be generated as {cell}_{name} from these IDs
  repeated clustermetadata.ID standby_ids = 4;

  // Whether to reload configuration immediately
  bool reload_config = 5;

  // Force configuration even if history record write times out.
  // When true, treats write timeouts as warnings (useful for tests and emergency operations).
  // When false, fails if history write blocks on sync replication (normal production behavior).
  bool force = 6;
}

message ConfigureSynchronousReplicationResponse {
}

// State gets the current status of the manager
message StateRequest {
}

message StateResponse {
  // Manager state (starting, ready, error)
  string state = 1;

  // Error message if state is error
  string error_message = 2;
}

// UpdateSynchronousStandbyList updates the synchronous standby list
message UpdateSynchronousStandbyListRequest {
  // Operation to perform (add, remove, or replace)
  StandbyUpdateOperation operation = 1;

  // List of standby multipooler IDs to add/remove/replace
  // The application names will be generated as {cell}_{name} from these IDs
  repeated clustermetadata.ID standby_ids = 2;

  // Whether to reload configuration immediately
  bool reload_config = 3;

  // Consensus term (used by MultiOrch for term validation)
  int64 consensus_term = 4;

  // Force the operation even if the term doesn't match
  bool force = 5;

  // Coordinator ID that initiated this operation (for audit trail)
  clustermetadata.ID coordinator_id = 6;
}

message UpdateSynchronousStandbyListResponse {
}

// ConsensusTerm represents the consensus term information for the pooler
// This is persisted to disk at $PGDATA/consensus/consensus_term.json
message ConsensusTerm {
  // Term number for this consensus term
  int64 term_number = 1;

  // ID of the coordinator (multiorch) that this node accepted the term from
  clustermetadata.ID accepted_term_from_coordinator_id = 2;

  // Timestamp of the last acceptance
  google.protobuf.Timestamp last_acceptance_time = 3;

  // ID of the leader of the current term
  clustermetadata.ID leader_id = 4;

  // The term for which this pooler was promoted to primary.
  // Set during promotion (InitializeEmptyPrimary or Promote).
  // Preserved when consensus term increases (new elections).
  // Cleared to 0 when demoted (DemoteStalePrimary) or restored from backup.
  // 0 if never primary.
  int64 primary_term = 5;
}

// InitializeEmptyPrimary initializes this pooler as an empty primary
// Used during bootstrap initialization of a new shard
message InitializeEmptyPrimaryRequest {
  // Consensus term to set for this primary
  int64 consensus_term = 1;

  // Optional: Create durability policy after initialization
  string durability_policy_name = 2;
  clustermetadata.QuorumRule durability_quorum_rule = 3;

  // ID of the multiorch coordinator performing the bootstrap
  string coordinator_id = 4;
}

message InitializeEmptyPrimaryResponse {
  // Whether the initialization succeeded
  bool success = 1;

  // Error message if initialization failed
  string error_message = 2;

  // Backup ID of the initial backup created during initialization
  // This can be used by standbys to restore from the same backup
  string backup_id = 3;
}

// Backup operations
// All of these RPCs perform backups and restores in the context
// of the database, table group, and shard this multipooler serves.

// BackupRequest requests a backup
message BackupRequest {
  // force_primary should typically be false, because backups should be done on
  // replicas to reduce load on the primary cluster. This is dangerous.
  bool force_primary = 1;

  // type indicates the type of backup that should be done. "full", "differential",
  // and "incremental" are examples that are appropriate for pgBackRest.
  string type = 2;

  // job_id is an optional tracking ID from MultiAdmin for backup identification.
  // If provided, stored as annotation on the backup for status queries.
  // If empty, multipooler generates one using the same format.
  // Format: YYYYMMDD-HHMMSS.microseconds[_suffix]
  string job_id = 3;
}

// BackupResponse contains the result of a backup operation
message BackupResponse {
  // backup_id uniquely identifies a backup and can be used to restore this
  // backup in the future.
  string backup_id = 1;
}

// RestoreFromBackupRequest requests a restore from a backup
message RestoreFromBackupRequest {
  // Backup to restore from. If this is empty, we restore from the latest
  // backup.
  string backup_id = 1;
}

// RestoreFromBackupResponse contains the result of a restore operation
message RestoreFromBackupResponse {
}

// GetBackupsRequest requests backup information
message GetBackupsRequest {
  uint32 limit = 1;
}

// GetBackupsResponse contains the list of backups
message GetBackupsResponse {
  repeated BackupMetadata backups = 1;
}

// GetBackupByJobIdRequest queries a backup by the job_id annotation.
message GetBackupByJobIdRequest {
  // The job_id to search for in backup annotations.
  string job_id = 1;
}

// GetBackupByJobIdResponse returns the backup matching the job_id.
// If no backup is found, backup will be nil.
message GetBackupByJobIdResponse {
  // The backup metadata if found, nil if not found.
  BackupMetadata backup = 1;
}

// BackupMetadata contains metadata about a backup
message BackupMetadata {
  string table_group = 1;
  string shard = 2;
  Status status = 3;
  string backup_id = 4;
  string final_lsn = 5; // Final checkpoint LSN (stop LSN) for this backup

  // Job ID annotation if present (for cross-process tracking)
  string job_id = 6;

  // Size of the backup in bytes (original database size before compression)
  uint64 backup_size_bytes = 7;

  // Type of backup: "full", "diff", or "incr" (as reported by pgbackrest)
  string type = 8;

  // Multipooler ID that created this backup (from pgbackrest annotation)
  string multipooler_id = 9;

  // Pooler type that created this backup (from pgbackrest annotation)
  clustermetadata.PoolerType pooler_type = 10;

  // Status represents the state of a backup
  enum Status {
    UNKNOWN = 0;
    INCOMPLETE = 1;
    COMPLETE = 2;
    // TODO: add VALID/INVALID states
  }
}

// =============================================================================
// Durability Policy APIs
// =============================================================================

// GetDurabilityPolicyRequest requests the active durability policy
message GetDurabilityPolicyRequest {
  // Empty - retrieves the currently active policy
}

// GetDurabilityPolicyResponse returns the active durability policy
message GetDurabilityPolicyResponse {
  // The active durability policy, or null if none is configured
  clustermetadata.DurabilityPolicy policy = 1;
}

// CreateDurabilityPolicyRequest creates a new durability policy
message CreateDurabilityPolicyRequest {
  // The name of the policy (e.g., "ANY_2", "MULTI_CELL_ANY_2")
  string policy_name = 1;

  // The quorum rule defining the policy behavior
  clustermetadata.QuorumRule quorum_rule = 2;
}

// CreateDurabilityPolicyResponse confirms policy creation
// Errors are returned via gRPC status codes, not in the response body
message CreateDurabilityPolicyResponse {
}

// =============================================================================
// PgRewind APIs
// =============================================================================

// RewindToSourceRequest requests pg_rewind to synchronize with a source server.
// This operation:
// 1. Stops PostgreSQL
// 2. Runs pg_rewind --dry-run to check if rewind is needed
// 3. If needed, runs actual pg_rewind
// 4. Starts PostgreSQL
message RewindToSourceRequest {
  // Source multipooler (the primary) to rewind to
  clustermetadata.MultiPooler source = 1;
}

message RewindToSourceResponse {
  // True if the operation completed successfully
  bool success = 1;

  // Error message if operation failed
  string error_message = 2;

  // True if servers had diverged and pg_rewind was performed
  // False if timelines were compatible and no rewind was needed
  bool rewind_performed = 3;
}

// =============================================================================
// PostgreSQL Monitoring Control APIs
// =============================================================================

// SetMonitorRequest enables or disables the PostgreSQL monitoring goroutine
message SetMonitorRequest {
  // Whether to enable monitoring (true) or disable it (false)
  bool enabled = 1;
}

// SetMonitorResponse confirms monitoring was updated
// Errors are returned via gRPC status codes, not in the response body
message SetMonitorResponse {
  // Empty - success indicated by no error
}
