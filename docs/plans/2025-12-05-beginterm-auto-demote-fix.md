# BeginTerm Auto-Demotion Fix Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Fix the consensus protocol gap where primaries don't demote themselves when accepting BeginTerm for a higher term, eliminating split-brain risk.

**Architecture:** When a primary accepts BeginTerm for a higher term, it will immediately demote itself (drain connections, checkpoint, restart as standby) and return the demote LSN in the response. This ensures the old primary stops accepting writes before any new primary is promoted.

**Tech Stack:** Go, gRPC, Protocol Buffers, PostgreSQL

---

## Background

The current BeginTerm implementation has a critical gap:
1. Primary accepts BeginTerm for higher term
2. Primary does NOT demote itself
3. Old primary continues accepting writes â†’ split-brain risk

The fix:
1. Extract demote logic into a method callable with action lock held
2. In BeginTerm, after accepting term, check if primary and demote
3. Return demote_lsn in response so coordinator knows the cutoff point

---

### Task 1: Add demote_lsn field to BeginTermResponse proto

**Files:**
- Modify: `proto/consensusdata.proto:57-66`

**Step 1: Update the proto file**

Edit `proto/consensusdata.proto` to add the demote_lsn field:

```protobuf
message BeginTermResponse {
  // Current term, for candidate to update itself
  int64 term = 1;

  // True if the term was accepted
  bool accepted = 2;

  // ID of the responding pooler
  string pooler_id = 3;

  // If this node was a primary and demoted itself, this contains the final LSN
  // before demotion. Standbys should replicate up to this point before switching
  // to a new primary. Empty if node was not a primary or didn't demote.
  string demote_lsn = 4;

  // Role of this node before processing BeginTerm (primary/replica)
  string previous_role = 5;
}
```

**Step 2: Regenerate protobuf code**

Run: `make proto`
Expected: SUCCESS, generated files updated

**Step 3: Verify build**

Run: `go build ./...`
Expected: SUCCESS

---

### Task 2: Extract demote core logic into demoteLocked method

**Files:**
- Modify: `go/multipooler/manager/rpc_manager.go`

**Step 1: Create demoteLocked helper method**

Add this method after the existing `Demote` function (around line 910):

```go
// demoteLocked performs the core demotion logic.
// REQUIRES: action lock must already be held by the caller.
// This is used by BeginTerm to demote inline without re-acquiring the lock.
func (pm *MultiPoolerManager) demoteLocked(ctx context.Context, consensusTerm int64, drainTimeout time.Duration) (string, error) {
	pm.logger.InfoContext(ctx, "demoteLocked called",
		"consensus_term", consensusTerm,
		"drain_timeout", drainTimeout)

	// Check current demotion state
	state, err := pm.checkDemotionState(ctx)
	if err != nil {
		return "", err
	}

	// If everything is already complete, return the existing LSN (idempotent)
	if state.isServingReadOnly && state.isReplicaInTopology && state.isReadOnly {
		pm.logger.InfoContext(ctx, "Demotion already complete (idempotent)",
			"lsn", state.finalLSN)
		return state.finalLSN, nil
	}

	// Transition to Read-Only Serving
	if err := pm.setServingReadOnly(ctx, state); err != nil {
		return "", err
	}

	// Drain & Checkpoint (Parallel)
	if err := pm.drainAndCheckpoint(ctx, drainTimeout); err != nil {
		return "", err
	}

	// Terminate Remaining Write Connections
	connectionsTerminated, err := pm.terminateWriteConnections(ctx)
	if err != nil {
		// Log but don't fail - connections will eventually timeout
		pm.logger.WarnContext(ctx, "Failed to terminate write connections", "error", err)
	}

	// Capture State & Make PostgreSQL Read-Only
	finalLSN, err := pm.getPrimaryLSN(ctx)
	if err != nil {
		pm.logger.ErrorContext(ctx, "Failed to capture final LSN", "error", err)
		return "", err
	}

	if err := pm.restartPostgresAsStandby(ctx, state); err != nil {
		return "", err
	}

	// Reset Synchronous Replication Configuration
	if err := pm.resetSynchronousReplication(ctx); err != nil {
		pm.logger.WarnContext(ctx, "Failed to reset synchronous replication configuration", "error", err)
	}

	// Update Topology
	if err := pm.updateTopologyAfterDemotion(ctx, state); err != nil {
		return "", err
	}

	pm.logger.InfoContext(ctx, "demoteLocked completed successfully",
		"final_lsn", finalLSN,
		"consensus_term", consensusTerm,
		"connections_terminated", connectionsTerminated)

	return finalLSN, nil
}
```

**Step 2: Refactor Demote to use demoteLocked**

Modify the existing `Demote` function to call `demoteLocked`:

```go
func (pm *MultiPoolerManager) Demote(ctx context.Context, consensusTerm int64, drainTimeout time.Duration, force bool) (*multipoolermanagerdatapb.DemoteResponse, error) {
	if err := pm.checkReady(); err != nil {
		return nil, err
	}

	// Acquire the action lock to ensure only one mutation runs at a time
	ctx, err := pm.actionLock.Acquire(ctx, "Demote")
	if err != nil {
		return nil, err
	}
	defer pm.actionLock.Release(ctx)

	pm.logger.InfoContext(ctx, "Demote called",
		"consensus_term", consensusTerm,
		"drain_timeout", drainTimeout,
		"force", force)

	// Validate and update consensus term
	if err = pm.validateAndUpdateTerm(ctx, consensusTerm, force); err != nil {
		return nil, err
	}

	// Guard rail: Demote can only be called on a PRIMARY
	if err := pm.checkPrimaryGuardrails(ctx); err != nil {
		return nil, err
	}

	// Check if already demoted (for idempotency response)
	state, err := pm.checkDemotionState(ctx)
	if err != nil {
		return nil, err
	}
	if state.isServingReadOnly && state.isReplicaInTopology && state.isReadOnly {
		return &multipoolermanagerdatapb.DemoteResponse{
			WasAlreadyDemoted:     true,
			ConsensusTerm:         consensusTerm,
			LsnPosition:           state.finalLSN,
			ConnectionsTerminated: 0,
		}, nil
	}

	// Perform core demotion
	finalLSN, err := pm.demoteLocked(ctx, consensusTerm, drainTimeout)
	if err != nil {
		return nil, err
	}

	return &multipoolermanagerdatapb.DemoteResponse{
		WasAlreadyDemoted:     false,
		ConsensusTerm:         consensusTerm,
		LsnPosition:           finalLSN,
		ConnectionsTerminated: 0, // Already logged in demoteLocked
	}, nil
}
```

**Step 3: Verify build**

Run: `go build ./go/multipooler/...`
Expected: SUCCESS

---

### Task 3: Add auto-demotion to BeginTerm

**Files:**
- Modify: `go/multipooler/manager/rpc_consensus.go:28-118`

**Step 1: Update BeginTerm to demote primaries**

Replace the BeginTerm function with:

```go
// BeginTerm handles coordinator requests during leader appointments.
// If this node is a primary and accepts a higher term, it will automatically
// demote itself to prevent split-brain scenarios.
func (pm *MultiPoolerManager) BeginTerm(ctx context.Context, req *consensusdatapb.BeginTermRequest) (*consensusdatapb.BeginTermResponse, error) {
	// Acquire the action lock to ensure only one consensus operation runs at a time
	// This prevents split-brain acceptance and ensures term updates are serialized
	var err error
	ctx, err = pm.actionLock.Acquire(ctx, "BeginTerm")
	if err != nil {
		return nil, err
	}
	defer pm.actionLock.Release(ctx)

	// CRITICAL: Must be able to reach Postgres to participate in cohort
	if pm.db == nil {
		return nil, fmt.Errorf("postgres unreachable, cannot accept new term")
	}

	// Test database connectivity
	if err = pm.db.PingContext(ctx); err != nil {
		return nil, fmt.Errorf("postgres unhealthy, cannot accept new term: %w", err)
	}

	// Check if we're currently a primary (before any term changes)
	wasPrimary, err := pm.isPrimary(ctx)
	if err != nil {
		// If we can't determine role, log warning but continue
		pm.logger.WarnContext(ctx, "Failed to determine if primary", "error", err)
		wasPrimary = false
	}

	previousRole := "replica"
	if wasPrimary {
		previousRole = "primary"
	}

	// Get current consensus state
	pm.mu.Lock()
	cs := pm.consensusState
	pm.mu.Unlock()

	if cs == nil {
		return nil, fmt.Errorf("consensus state not initialized")
	}

	currentTerm, err := cs.GetCurrentTermNumber(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get current term: %w", err)
	}

	term, err := cs.GetTerm(ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to get term: %w", err)
	}

	response := &consensusdatapb.BeginTermResponse{
		Term:         currentTerm,
		Accepted:     false,
		PoolerId:     pm.serviceID.GetName(),
		DemoteLsn:    "",
		PreviousRole: previousRole,
	}

	// Reject if term is outdated
	if req.Term < currentTerm {
		return response, nil
	}

	// Check if we've already accepted this term from a different coordinator
	if req.Term == currentTerm && term != nil && term.AcceptedTermFromCoordinatorId != nil {
		// Compare full ID (component, cell, name) not just name
		if !proto.Equal(term.AcceptedTermFromCoordinatorId, req.CandidateId) {
			return response, nil
		}
	}

	// Update term if needed (only if req.Term > currentTerm)
	// This will reset AcceptedTermFromCoordinatorId to nil
	if req.Term > currentTerm {
		if err := pm.validateAndUpdateTerm(ctx, req.Term, false); err != nil {
			return nil, fmt.Errorf("failed to update term: %w", err)
		}
		response.Term = req.Term
	}

	// Check if we're caught up with replication (within 30 seconds)
	// Only relevant for standbys
	if !wasPrimary {
		var lastMsgReceiptTime *time.Time
		err = pm.db.QueryRowContext(ctx, "SELECT last_msg_receipt_time FROM pg_stat_wal_receiver").Scan(&lastMsgReceiptTime)
		if err != nil {
			// No WAL receiver (could be disconnected standby)
			// Don't reject the acceptance - let it proceed
		} else if lastMsgReceiptTime != nil {
			timeSinceLastMessage := time.Since(*lastMsgReceiptTime)
			if timeSinceLastMessage > 30*time.Second {
				// We're too far behind in replication, don't accept
				return response, nil
			}
		}
	}

	// Accept term from coordinator atomically (checks, saves to disk, updates memory)
	if err := cs.AcceptCandidateAndSave(ctx, req.CandidateId); err != nil {
		return nil, fmt.Errorf("failed to accept term from coordinator: %w", err)
	}

	response.Accepted = true

	// CRITICAL: If we were a primary and accepted a higher term, demote immediately.
	// This prevents split-brain by ensuring the old primary stops accepting writes
	// before any new primary is promoted.
	if wasPrimary && req.Term > currentTerm {
		pm.logger.InfoContext(ctx, "Primary accepting higher term, initiating auto-demotion",
			"old_term", currentTerm,
			"new_term", req.Term,
			"candidate_id", req.CandidateId.GetName())

		// Use a reasonable drain timeout for auto-demotion
		drainTimeout := 5 * time.Second
		demoteLSN, demoteErr := pm.demoteLocked(ctx, req.Term, drainTimeout)
		if demoteErr != nil {
			// Log the error but still return accepted=true
			// The term was accepted, demotion is a best-effort cleanup
			pm.logger.ErrorContext(ctx, "Auto-demotion failed after accepting term",
				"error", demoteErr,
				"term", req.Term)
			// Don't fail the BeginTerm - the term acceptance is the critical part
			// The coordinator can call Demote explicitly if needed
		} else {
			response.DemoteLsn = demoteLSN
			pm.logger.InfoContext(ctx, "Auto-demotion completed successfully",
				"demote_lsn", demoteLSN,
				"term", req.Term)
		}
	}

	return response, nil
}
```

**Step 2: Verify build**

Run: `go build ./go/multipooler/...`
Expected: SUCCESS

---

### Task 4: Add unit test for BeginTerm auto-demotion

**Files:**
- Modify: `go/multipooler/manager/rpc_consensus_test.go`

**Step 1: Add test for auto-demotion behavior**

Add the following test to `rpc_consensus_test.go`:

```go
func TestBeginTerm_AutoDemotesPrimary(t *testing.T) {
	// This test verifies that when a primary accepts BeginTerm for a higher term,
	// it automatically demotes itself to prevent split-brain.

	// Note: This is a behavioral specification test. The actual demotion logic
	// requires a full PostgreSQL instance, so this test focuses on verifying:
	// 1. The response includes previous_role
	// 2. For primaries accepting higher terms, demotion is attempted

	// Full integration testing is done in go/test/endtoend/beginterm_demote_test.go
	t.Skip("Full test requires PostgreSQL instance - see beginterm_demote_test.go")
}
```

**Step 2: Verify tests pass**

Run: `go test -v ./go/multipooler/manager/... -run TestBeginTerm`
Expected: SKIP with message about requiring PostgreSQL

---

### Task 5: Update end-to-end test to verify fix

**Files:**
- Modify: `go/test/endtoend/beginterm_demote_test.go`

**Step 1: Update test assertions to verify fix works**

Change the `demonstrates_bug_primary_not_demoted` subtest:

```go
	// Step 6: After fix: Primary SHOULD demote after accepting higher term
	t.Run("verify_primary_demoted", func(t *testing.T) {
		stillPrimary := isPostgresPrimary(t, primaryNode)

		// After fix: Primary should have demoted itself
		assert.False(t, stillPrimary,
			"Primary %s should have demoted after accepting term %d",
			primaryNode.name, newTerm)

		if !stillPrimary {
			t.Logf("SUCCESS: Primary %s correctly demoted after accepting BeginTerm with term %d", primaryNode.name, newTerm)
		} else {
			t.Errorf("FAILED: Primary %s is still running as primary after accepting term %d", primaryNode.name, newTerm)
		}
	})
```

Change the `demonstrates_split_brain_risk` subtest:

```go
	// Step 7: After fix: Old primary should reject writes
	t.Run("verify_no_split_brain_risk", func(t *testing.T) {
		socketDir := filepath.Join(primaryNode.dataDir, "pg_sockets")
		db := connectToPostgres(t, socketDir, primaryNode.pgPort)
		defer db.Close()

		// After demotion, the old primary is now a standby and should reject writes
		_, err := db.Exec("CREATE TABLE IF NOT EXISTS split_brain_test (id serial PRIMARY KEY, value text)")

		// After fix: This should fail because the node is now read-only
		if err != nil {
			t.Logf("SUCCESS: Old primary correctly rejects writes after demotion: %v", err)
			assert.Contains(t, err.Error(), "read-only", "Error should indicate read-only mode")
		} else {
			t.Errorf("FAILED: Old primary still accepts writes after accepting higher term - split-brain risk!")
		}
	})
```

**Step 2: Run the updated test**

Run: `PATH=/Users/deepthi/.gvm/gos/go1.25.0/bin:/Applications/Postgres.app/Contents/Versions/18/bin:/Users/deepthi/github/multigres/bin:$PATH MTROOT=/Users/deepthi/github/multigres go test -v -run TestBeginTermDemoteGap -timeout 5m ./go/test/endtoend/...`

Expected: PASS with messages:
- "SUCCESS: Primary node0 correctly demoted after accepting BeginTerm with term 2"
- "SUCCESS: Old primary correctly rejects writes after demotion"

---

### Task 6: Run full test suite

**Files:**
- Test: All related test files

**Step 1: Run unit tests**

Run: `go test -v ./go/multipooler/manager/...`
Expected: PASS

**Step 2: Run end-to-end tests**

Run: `PATH=/Users/deepthi/.gvm/gos/go1.25.0/bin:/Applications/Postgres.app/Contents/Versions/18/bin:/Users/deepthi/github/multigres/bin:$PATH MTROOT=/Users/deepthi/github/multigres go test -v -timeout 10m ./go/test/endtoend/...`
Expected: PASS for all tests

---

## Summary

This implementation:

1. **Adds demote_lsn to BeginTermResponse** - Allows coordinator to know the cutoff LSN for standbys
2. **Extracts demoteLocked helper** - Reusable demotion logic callable with action lock held
3. **Auto-demotes primaries in BeginTerm** - When a primary accepts a higher term, it immediately demotes
4. **Updates tests** - End-to-end test verifies the fix works

The key insight is that demotion happens synchronously within BeginTerm, ensuring the old primary stops accepting writes BEFORE the coordinator proceeds with promoting a new primary. This eliminates the split-brain window.
